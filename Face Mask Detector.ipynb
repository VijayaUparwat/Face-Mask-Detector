{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = r\"C:\\Users\\Siddharth\\Desktop\\logo\\ammm\\tmmm\\dataset\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "categories = os.listdir(data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = [i for i in range (len(categories))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_dict = dict(zip(categories,labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'with mask': 0, 'without mask': 1}\n"
     ]
    }
   ],
   "source": [
    "print(label_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['with mask', 'without mask']\n"
     ]
    }
   ],
   "source": [
    "print(categories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 1]\n"
     ]
    }
   ],
   "source": [
    "print(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_size = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=[]\n",
    "target=[]\n",
    "for category in categories:\n",
    "    folder_path = os.path.join(data_path, category)\n",
    "    img_names = os.listdir(folder_path)\n",
    "    \n",
    "    for img_name in img_names:\n",
    "        img_path = os.path.join(folder_path, img_name)\n",
    "        img = cv2.imread(img_path)\n",
    "        \n",
    "        try:\n",
    "            gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "            resized = cv2.resize(gray,(img_size,img_size))\n",
    "            data.append(resized)\n",
    "            target.append(label_dict[category])\n",
    "        except Exception as e:\n",
    "            print('Exception :',e)\n",
    "                \n",
    "    \n",
    "           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.array(data)/255.0\n",
    "data = np.reshape(data,(data.shape[0],img_size,img_size,1))\n",
    "target= np.array(target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils import np_utils\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_target = tf.keras.utils.to_categorical(target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       ...,\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.]], dtype=float32)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_target\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('data', data)\n",
    "np.save('target', new_target )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "##CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.load('data.npy')\n",
    "target = np.load('target.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Flatten, Dropout\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras.callbacks import ModelCheckpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(Conv2D(200,(3,3), input_shape = data.shape[1:] ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size = (2,2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(Conv2D(100,(3,3)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size = (2,2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(Flatten())\n",
    "model.add(Dropout(0.5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(Dense(50, activation = 'relu'))\n",
    "model.add(Dense(2, activation = 'softmax'))\n",
    "model.compile(loss = 'categorical_crossentropy', optimizer = 'adam', metrics= ['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, test_data, train_target, test_target= train_test_split(data,target,test_size=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = ModelCheckpoint('model-{epoch:03d}.model', monitor = 'val_loss',verbose=0,save_best_only =True, mode='auto')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "31/31 [==============================] - ETA: 0s - loss: 0.7114 - accuracy: 0.5515WARNING:tensorflow:From c:\\users\\siddharth\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\tensorflow\\python\\training\\tracking\\tracking.py:111: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This property should not be used in TensorFlow 2.0, as updates are applied automatically.\n",
      "WARNING:tensorflow:From c:\\users\\siddharth\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\tensorflow\\python\\training\\tracking\\tracking.py:111: Layer.updates (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This property should not be used in TensorFlow 2.0, as updates are applied automatically.\n",
      "INFO:tensorflow:Assets written to: model-001.model\\assets\n",
      "31/31 [==============================] - 80s 3s/step - loss: 0.7114 - accuracy: 0.5515 - val_loss: 0.6800 - val_accuracy: 0.4879\n",
      "Epoch 2/20\n",
      "31/31 [==============================] - ETA: 0s - loss: 0.6023 - accuracy: 0.6768INFO:tensorflow:Assets written to: model-002.model\\assets\n",
      "31/31 [==============================] - 80s 3s/step - loss: 0.6023 - accuracy: 0.6768 - val_loss: 0.5557 - val_accuracy: 0.6694\n",
      "Epoch 3/20\n",
      "31/31 [==============================] - ETA: 0s - loss: 0.4334 - accuracy: 0.8202INFO:tensorflow:Assets written to: model-003.model\\assets\n",
      "31/31 [==============================] - 80s 3s/step - loss: 0.4334 - accuracy: 0.8202 - val_loss: 0.4026 - val_accuracy: 0.8065\n",
      "Epoch 4/20\n",
      "31/31 [==============================] - ETA: 0s - loss: 0.3329 - accuracy: 0.8646INFO:tensorflow:Assets written to: model-004.model\\assets\n",
      "31/31 [==============================] - 79s 3s/step - loss: 0.3329 - accuracy: 0.8646 - val_loss: 0.2796 - val_accuracy: 0.9113\n",
      "Epoch 5/20\n",
      "31/31 [==============================] - ETA: 0s - loss: 0.2303 - accuracy: 0.9141INFO:tensorflow:Assets written to: model-005.model\\assets\n",
      "31/31 [==============================] - 80s 3s/step - loss: 0.2303 - accuracy: 0.9141 - val_loss: 0.2266 - val_accuracy: 0.9274\n",
      "Epoch 6/20\n",
      "31/31 [==============================] - ETA: 0s - loss: 0.1655 - accuracy: 0.9394INFO:tensorflow:Assets written to: model-006.model\\assets\n",
      "31/31 [==============================] - 78s 3s/step - loss: 0.1655 - accuracy: 0.9394 - val_loss: 0.1811 - val_accuracy: 0.9395\n",
      "Epoch 7/20\n",
      "31/31 [==============================] - 76s 2s/step - loss: 0.1553 - accuracy: 0.9414 - val_loss: 0.2450 - val_accuracy: 0.8911\n",
      "Epoch 8/20\n",
      "31/31 [==============================] - 75s 2s/step - loss: 0.1426 - accuracy: 0.9465 - val_loss: 0.2236 - val_accuracy: 0.9194\n",
      "Epoch 9/20\n",
      "31/31 [==============================] - ETA: 0s - loss: 0.1009 - accuracy: 0.9626INFO:tensorflow:Assets written to: model-009.model\\assets\n",
      "31/31 [==============================] - 77s 2s/step - loss: 0.1009 - accuracy: 0.9626 - val_loss: 0.1670 - val_accuracy: 0.9395\n",
      "Epoch 10/20\n",
      "31/31 [==============================] - 75s 2s/step - loss: 0.0768 - accuracy: 0.9737 - val_loss: 0.1704 - val_accuracy: 0.9355\n",
      "Epoch 11/20\n",
      "31/31 [==============================] - 75s 2s/step - loss: 0.0668 - accuracy: 0.9717 - val_loss: 0.2061 - val_accuracy: 0.9315\n",
      "Epoch 12/20\n",
      "31/31 [==============================] - ETA: 0s - loss: 0.0857 - accuracy: 0.9626INFO:tensorflow:Assets written to: model-012.model\\assets\n",
      "31/31 [==============================] - 77s 2s/step - loss: 0.0857 - accuracy: 0.9626 - val_loss: 0.1562 - val_accuracy: 0.9435\n",
      "Epoch 13/20\n",
      "31/31 [==============================] - ETA: 0s - loss: 0.0401 - accuracy: 0.9879INFO:tensorflow:Assets written to: model-013.model\\assets\n",
      "31/31 [==============================] - 78s 3s/step - loss: 0.0401 - accuracy: 0.9879 - val_loss: 0.1469 - val_accuracy: 0.9516\n",
      "Epoch 14/20\n",
      "31/31 [==============================] - 75s 2s/step - loss: 0.0325 - accuracy: 0.9889 - val_loss: 0.1661 - val_accuracy: 0.9435\n",
      "Epoch 15/20\n",
      "31/31 [==============================] - 75s 2s/step - loss: 0.0322 - accuracy: 0.9879 - val_loss: 0.2152 - val_accuracy: 0.9435\n",
      "Epoch 16/20\n",
      "31/31 [==============================] - 75s 2s/step - loss: 0.0244 - accuracy: 0.9939 - val_loss: 0.2947 - val_accuracy: 0.9315\n",
      "Epoch 17/20\n",
      "31/31 [==============================] - 76s 2s/step - loss: 0.0414 - accuracy: 0.9848 - val_loss: 0.2575 - val_accuracy: 0.9476\n",
      "Epoch 18/20\n",
      "31/31 [==============================] - 76s 2s/step - loss: 0.0351 - accuracy: 0.9889 - val_loss: 0.1788 - val_accuracy: 0.9435\n",
      "Epoch 19/20\n",
      "31/31 [==============================] - 75s 2s/step - loss: 0.0266 - accuracy: 0.9919 - val_loss: 0.1895 - val_accuracy: 0.9395\n",
      "Epoch 20/20\n",
      "31/31 [==============================] - 75s 2s/step - loss: 0.0256 - accuracy: 0.9899 - val_loss: 0.1882 - val_accuracy: 0.9395\n"
     ]
    }
   ],
   "source": [
    "history= model.fit(train_data,train_target,epochs=20,callbacks=[checkpoint],validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5/5 [==============================] - 2s 434ms/step - loss: 0.1127 - accuracy: 0.9565\n",
      "[0.11267417669296265, 0.95652174949646]\n"
     ]
    }
   ],
   "source": [
    "print(model.evaluate(test_data,test_target))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "face_classifier = cv2.CascadeClassifier(r'C:\\Users\\Siddharth\\Desktop\\logo\\ammm\\tmmmharrcascade_frontalface_default.xml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "source = cv2.VideoCapture(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_dict = {0:'Mask',1:'No Mask'}\n",
    "coloe_dict = {0:(0,255,0),1:(0,0,255)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "while (True):\n",
    "    ret, img = source.read()\n",
    "    gray = cv2.cvtColor(img,cv2.COLOR_BGR2GRAY)\n",
    "    faces= face_classifier.detectMultiScale(gray,1.3,5)\n",
    "    \n",
    "    for x, y, w, h in faces:\n",
    "        face_img = gray[y:y+w, x:x+w]\n",
    "        resized= cv2.resize(face_img,(100,100))\n",
    "        normalized=resized/255.0\n",
    "        reshaped=np.reshape(normalized,(1,100,100,1))\n",
    "        result= model.predict(reshaped)\n",
    "        label= np.argmax(result,axis=1)[0]\n",
    "        cv2.rectangle(img,(x,y),(x+w, y+h),coloe_dict[label],2)\n",
    "        cv2.rectangle(img,(x,y-40),(x+w, y),coloe_dict[label],-1)\n",
    "        cv2.putText(img, label_dict[label],(x, y-10),cv2.FONT_HERSHEY_SIMPLEX,0.8,(255,255,255),2)\n",
    "        cv2.imshow('LIVE', img)\n",
    "        key=cv2.waitKey(1)\n",
    "        \n",
    "        if(key==27):\n",
    "           # break\n",
    "   # cv2.desroyAllWindows()\n",
    "    #source.release()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
